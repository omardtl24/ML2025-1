{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2eba98",
   "metadata": {},
   "source": [
    "# Practice Problems 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5116f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bedc5c8",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427f7f7",
   "metadata": {},
   "source": [
    "### Exercise 1.a (12.4 pg 404)\n",
    "\n",
    "Consider two independent random vectors $\\textbf{a}$ and $\\textbf{b}$ each of dimension $D$ and each being drawn from a Gaussian distribution with zero mean and unit variance. Show that the expected value of $(\\textbf{a}^{T}\\textbf{b})^2$ is given by $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e380426",
   "metadata": {},
   "source": [
    "From the information known, we can say the following:\n",
    "\n",
    "$$\\mathbb{E}[\\textbf{b}] = \\vec{\\mathbf{0}}_{D} \\quad \\equiv \\quad \\forall_{1 \\leq i \\leq D} \\,\\, \\mathbb{E}[b_i] = 0$$\n",
    "$$\\mathbb{E}[\\textbf{a}] = \\vec{\\mathbf{0}}_{D} \\quad \\equiv \\quad \\forall_{1 \\leq i \\leq D} \\,\\, \\mathbb{E}[a_i] = 0$$\n",
    "\n",
    "$$\\text{Var}[\\textbf{a}] = \\mathbb{E}[(\\textbf{a}-\\mathbb{E}[\\textbf{a}])^2] = \\mathbb{E}[(\\textbf{a})^2] = \\mathbf{I}_D $$\n",
    "$$\\text{Var}[\\textbf{b}] = \\mathbb{E}[(\\textbf{b}-\\mathbb{E}[\\textbf{b}])^2] = \\mathbb{E}[(\\textbf{b})^2] = \\mathbf{I}_D $$\n",
    "\n",
    "$$\\mathbb{E}[(\\textbf{a})^2] = \\mathbf{I}_D \\quad \\equiv \\quad \\forall_{1 \\leq i,j \\leq D} \\,\\, \\mathbb{E}[a_i a_j] = \\left\\{\n",
    "\\begin{array}{ccl}\n",
    "    1 & \\text{if} & i=j \\\\\n",
    "    0 & \\text{if} & i\\neq j\n",
    "\\end{array}\\right. \\quad \\quad (1)$$\n",
    "\n",
    "$$\\mathbb{E}[(\\textbf{b})^2] = \\mathbf{I}_D \\quad \\equiv \\quad \\forall_{1 \\leq i,j \\leq D} \\,\\, \\mathbb{E}[b_i b_j] = \\left\\{\n",
    "\\begin{array}{ccl}\n",
    "    1 & \\text{if} & i=j \\\\\n",
    "    0 & \\text{if} & i\\neq j\n",
    "\\end{array}\\right. \\quad \\quad (2)$$\n",
    "\n",
    "Also, we can say in the matrix notation, given that $\\textbf{a}$ and $\\textbf{b}$ are vectors of same dimension, that:\n",
    "\n",
    "$$\\textbf{a}^{T}\\textbf{b} = \\sum_{i=1} ^ {D} a_i \\cdot b_i$$\n",
    "\n",
    "Also, as we are going to take into account that if $x$ and $y$ are two random independent variables, we can say that:\n",
    "\n",
    "$$\\mathbb{E}[x \\cdot y] = \\mathbb{E}[x] \\cdot \\mathbb{E}[y]$$\n",
    "\n",
    "This last property can be generalized to the vector level operations in terms of $\\textbf{a}$ and $\\textbf{b}$, giving the following:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccl}\n",
    "    \\mathbb{E}[\\textbf{a}^{T}\\textbf{b}] &=& \\mathbb{E}[\\sum_{i=1} ^ {D} a_i \\cdot b_i] \\\\ \\\\\n",
    "                                         &=& \\sum_{i=1} ^ {D} \\mathbb{E}[a_i \\cdot b_i] \\\\ \\\\\n",
    "                                         &=& \\sum_{i=1} ^ {D} \\mathbb{E}[a_i] \\cdot \\mathbb{E}[b_i] \\\\ \\\\\n",
    "                                         &=& \\mathbb{E}[\\textbf{a}]^{T}\\mathbb{E}[\\textbf{b}]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Doing this same excercise with the corresponding $\\textbf{a}$ and $\\textbf{b}$ probability distributions we get the following:\n",
    "\n",
    "$$\\mathbb{E}[\\textbf{a}^{T}\\textbf{b}] = \\mathbb{E}[\\textbf{a}]^{T}\\mathbb{E}[\\textbf{b}] = \\vec{\\mathbf{0}}_{D}^{T} \\vec{\\mathbf{0}}_{D} = \\sum_{i=1} ^ {D} 0 \\cdot 0 = 0$$\n",
    "\n",
    "Using this middle result, we are going to look for the variance of the product of this two random variable vectors:\n",
    "\n",
    "$$\\text{Var}[\\textbf{a}^{T} \\textbf{b}] = \\mathbb{E}[(\\textbf{a}^{T} \\textbf{b}-\\mathbb{E}[\\textbf{a}^{T} \\textbf{b}])^2] = \\mathbb{E}[(\\textbf{a}^{T} \\textbf{b}-0)^2] = \\mathbb{E}[(\\textbf{a}^{T} \\textbf{b})^2]$$\n",
    "\n",
    "Here, we can see that the target value we want to find is the variance of the dot product of the random vectors. Now we will focus in solving the requested problem:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cclcr}\n",
    "    \\mathbb{E}[(\\textbf{a}^{T} \\textbf{b})^2] & = &  \\mathbb{E}[(\\sum_{i=1} ^ {D} a_i b_i)^2] & & \\text{Dot product definition}\\\\ \\\\\n",
    "                                              & = &  \\mathbb{E}[\\sum_{i=1} ^ {D} \\sum_{j=1} ^ {D} a_i b_i a_j b_j] & & \\text{Square of sum}\\\\ \\\\\n",
    "                                              & = &  \\mathbb{E}[\\sum_{i=1} ^ {D} \\sum_{j=1} ^ {D} a_i  a_j b_i b_j] & & \\text{Commutative property}\\\\ \\\\\n",
    "                                              & = &  \\sum_{i=1} ^ {D} \\sum_{j=1} ^ {D} \\mathbb{E}[a_i  a_j] \\mathbb{E}[b_i b_j] & & \\text{Expected value of} \\\\\n",
    "                                              & & & &\\text{independent variables}\\\\\n",
    "                                              & = &  \\sum_{i=1} ^ {D} 1 \\cdot 1 & & \\text{Apply } (1) \\text{ and }(2)\\\\ \\\\\n",
    "                                              & = & \\sum_{i=1} ^ {D} 1 = D & & \\text{Q.E.D}\n",
    "\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e141fcf",
   "metadata": {},
   "source": [
    "### Exercise 1.b (12.6 pg 404)\n",
    "\n",
    "Express the self-attention function:\n",
    "\n",
    "$$\\textbf{Y} = \\text{Attention}(\\textbf{Q}, \\textbf{K}, \\textbf{V}) = \\text{Softmax}\\left[ \\dfrac{\\textbf{Q}\\textbf{K}^{T}}{\\sqrt{D_k}} \\right] \\textbf{V}$$\n",
    "\n",
    "as a fully connected network in the form of a matrix that maps the full input sequence of concatenated word vectors into an output vector of the same dimension. Note that such a matrix would have $O(N^2D^2)$ parameters. Show that the self-attention network corresponds to a sparse version of this matrix with parameter sharing. Draw a sketch showing the structure of this matrix, indicating which blocks of parameters are shared and which blocks have all elements equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7dcf31",
   "metadata": {},
   "source": [
    "In order to do the first task, we will express the the query, key and value matrices as a dot product between the words input expressed as $X$ with a weights matrix:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{Q} &= \\mathbf{X} \\mathbf{W}^{(q)} \\\\\n",
    "\\mathbf{K} &= \\mathbf{X} \\mathbf{W}^{(k)} \\\\\n",
    "\\mathbf{V} &= \\mathbf{X} \\mathbf{W}^{(v)}\n",
    "\\end{align*}$$\n",
    "\n",
    "In this scenario, we will use a compact definition of the attention coefficients, defining:\n",
    "\n",
    "$$A = \\text{Softmax}\\left[ \\dfrac{\\mathbf{X} \\mathbf{W}^{(q)} (\\mathbf{X} \\mathbf{W}^{(k)})^{T}}{\\sqrt{D_k}} \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15ff4c",
   "metadata": {},
   "source": [
    "### Exercise 1.c (12.7 pg 404)\n",
    "\n",
    "Show that if we omit the positional encoding of the input vectors then the outputs of a multi-head attention layer defined by:\n",
    "\n",
    "$$\\textbf{Y(X)} = \\text{Concat}[\\textbf{H}_1 , ... , \\textbf{H}_H] \\textbf{W}^{(o)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{Q}_h &= \\mathbf{X} \\mathbf{W}_h^{(q)} \\\\\n",
    "\\mathbf{K}_h &= \\mathbf{X} \\mathbf{W}_h^{(k)} \\\\\n",
    "\\mathbf{V}_h &= \\mathbf{X} \\mathbf{W}_h^{(v)}\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\\mathbf{H}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h)$$\n",
    "\n",
    "are equivalent with respect to a reordering of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b5980",
   "metadata": {},
   "source": [
    "In order to proof this order relationship, we will define a permutation matrix $P$, the idea is to show that a permutation of the input relies in the same permutation in the transformer output. In a formal way (and defining the transformer as a function $\\mathcal{T}(X)$) we want to prove that:\n",
    "\n",
    "$$\\mathcal{T}(PX) = P \\cdot \\mathcal{T}(X)$$\n",
    "\n",
    "In order to do this, we will first try to define a generalized form of $\\mathcal{T}(X)$ and check if the property said holds.\n",
    "\n",
    "First, we will define the attention layers using the permutation matrix and we will try to see if the attention layer holds the property.\n",
    "\n",
    "We will use a important property, as softmax is applied per row, we can permut the rows if the result of the operation is given  by the permutation if the softmax output. In other words:\n",
    "\n",
    "$$\\text{Softmax}(PX) = P \\, \\text{Softmax}(X)$$\n",
    "\n",
    "Also, we will take into account that a permutation matrix is orthogonal, leading to:\n",
    "\n",
    "$$P^T = P^{-1} \\implies PP^T = I$$\n",
    "\n",
    "Given $PX$, the attention layer h will be written as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cclcr}\n",
    "    \\text{Attention}(P\\mathbf{Q}_h,P\\mathbf{K}_h, P\\mathbf{V}_h) & = &\\text{Softmax}\\left[ \\dfrac{P\\textbf{Q}_h(P\\textbf{K}_h)^{T}}{\\sqrt{D_k}} \\right] P\\textbf{V}_h & & \\text{Attention layer}\\\\ \\\\\n",
    "     & = & \\text{Softmax}\\left[ \\dfrac{P \\mathbf{X} \\mathbf{W}_h^{(q)} \\left(P \\mathbf{X} \\mathbf{W}_h^{(k)} \\right)^T}{\\sqrt{D_k}} \\right] P \\mathbf{X} \\mathbf{W}_h^{(v)} & & \\text{Trainable definition} \\\\ \\\\\n",
    "    & = & \\text{Softmax}\\left[ \\dfrac{P \\mathbf{X} \\mathbf{W}_h^{(q)} \\mathbf{W}_h^{(k)T} \\mathbf{X}^T P^T}{\\sqrt{D_k}} \\right] P \\mathbf{X} \\mathbf{W}_h^{(v)} & & \\text{Transpose properties} \\\\ \\\\\n",
    "    & = & P \\cdot \\text{Softmax}\\left[ \\dfrac{ \\mathbf{X} \\mathbf{W}_h^{(q)} \\mathbf{W}_h^{(k)T} \\mathbf{X}^T }{\\sqrt{D_k}} \\right]P^T P \\mathbf{X} \\mathbf{W}_h^{(v)} & & \\text{Permutation + Softmax} \\\\ \\\\\n",
    "    & = & P \\cdot \\text{Softmax}\\left[ \\dfrac{ \\mathbf{X} \\mathbf{W}_h^{(q)} \\mathbf{W}_h^{(k)T} \\mathbf{X}^T }{\\sqrt{D_k}} \\right] \\mathbf{X} \\mathbf{W}_h^{(v)} & & \\text{Permutation properties} \\\\ \\\\\n",
    "    & = & P \\cdot \\text{Softmax}\\left[ \\dfrac{ \\mathbf{X} \\mathbf{W}_h^{(q)} (\\mathbf{X} \\mathbf{W}_h^{(k)} )^T }{\\sqrt{D_k}} \\right] \\mathbf{X} \\mathbf{W}_h^{(v)} & & \\text{Transpose properties} \\\\ \\\\\n",
    "    & = & P \\cdot \\text{Softmax}\\left[ \\dfrac{ \\mathbf{Q}_h \\mathbf{K}_h^T }{\\sqrt{D_k}} \\right] \\mathbf{V}_h & & \\text{Trainable definition} \\\\ \\\\\n",
    "    & = & P \\cdot \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) & & \\text{Attention layer}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We proved that a single attention layer holds the property of permutation. Then we'll use this midterm result over the multi-head layer in order to see that property holds.\n",
    "\n",
    "We will first define a partition over the weights matrix as it follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{(o)} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{W}_1^{(o)} \\\\\n",
    "\\mathbf{W}_2^{(o)} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{W}_H^{(o)}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\quad \\text{with each } \\mathbf{W}_i^{(o)} \\in \\mathbb{R}^{d_h \\times D}\n",
    "$$\n",
    "\n",
    "Then, we can define the product of the layer as it follows:\n",
    "\n",
    "$$\\textbf{Y(X)} = \\text{Concat}[\\textbf{H}_1 , ... , \\textbf{H}_H] \\textbf{W}^{(o)} = \\sum_{i=1}^{H} \\textbf{H}_i \\cdot \\mathbf{W}_i^{(o)}$$\n",
    "\n",
    "Now we will use the different matrix properties to see if the property holds:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cclcr}\n",
    "    \\textbf{Y}(P\\textbf{X}) &=& \\sum_{i=1}^{H} \\textbf{H}_i(P\\textbf{X}) \\cdot \\mathbf{W}_i^{(o)} & & \\text{Partition definition} \\\\ \\\\\n",
    "                            &=& \\sum_{i=1}^{H} P \\cdot\\textbf{H}_i(\\textbf{X}) \\cdot \\mathbf{W}_i^{(o)} & & \\text{Attention permutation} \\\\ \\\\\n",
    "                            &=& P \\cdot \\sum_{i=1}^{H} \\textbf{H}_i(\\textbf{X}) \\cdot \\mathbf{W}_i^{(o)} & & \\text{Linear independence} \\\\ \\\\\n",
    "                            &=& P \\cdot \\textbf{Y(X)} & &  \\text{Q.E.D}\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165caf9",
   "metadata": {},
   "source": [
    "### Exercise 1.d (12.15 pg 405)\n",
    "\n",
    "Consider a sequence of two tokens $y_1$ and $y_2$ each of which can take the states $A$ or $B$. The table below shows the joint probability distribution $p(y_1, y_2)$:\n",
    "\n",
    "| | $y_1 = A$ | $y_1 = B$ |\n",
    "|-|-|-|\n",
    "| $y_2 = A$ | 0.0 | 0.4 |\n",
    "| $y_2 = B$ | 0.1 | 0.25 |\n",
    "\n",
    "We see that the most probable sequence is $ y_1 = B, \\, y_2 = A$ and that this has probability 0.4. Using the sum and product rules of probability, write down the values of the marginal distribution $ p(y_1) $ and the conditional distribution $ p(y_2|y_1) $. Show that if we first maximize $ p(y_1) $ to give a value $ y_1^* $ and then subsequently maximize $ p(y_2|y_1^*) $ then we obtain a sequence that is different from the overall most probable sequence. Find the probability of the sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
